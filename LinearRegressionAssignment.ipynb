{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression analysis is a powerful statistical method that allows us to examine the relationship between two or more variables of interest.Three major uses for regression analysis are (1) determining the strength of predictors, (2) forecasting an effect, and (3) trend forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression  is a type of regression analysis in which we do the prediction of dependent variable based on independent variable. It will predict a quantitative response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhen to use Linear Regression? Explain the equation of a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use linear regression when we have only one dependent variable and one independent variable and the response we expect is a quantitative response\n",
    "The equation is y=mx+b where m is the intercept and b is the slop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat kind of plots will you use to showcase the relationship amongst the columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use correlation heat maps where a value above 6 is considred as highily coorelated.Another way to check the colinearity is VIF whre a value above 5 is considered as highly correlated.Yet another method is Principle component analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tHow is the best fit line chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best fit line is obtained by minimizing the residual. Residual is the distance between the actual Y and the predicted Y. Resudal formula is  ùëü=ùë¶‚àí(ùëöùë•+ùëè)where ùëöùë•+ùëè= predicted Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat is gradient descent, and why is it used? Explain the maths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm that's used when training a machine learning model. It's based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.\n",
    "\n",
    "The reason we use Gradient descent is that for linear regression Gradient descent has less computational complexity.\n",
    "\n",
    "Gradient descent use derivates to actually decide whether to increase or decrease the weights in order to increase or decrease any objective function.\n",
    "\n",
    "Primarily we shall be dealing with two concepts from calculus :\n",
    "\n",
    "    Power Rule\n",
    "\n",
    "Power rule calculates the derivative of a variable raised to a power.\n",
    "\n",
    "f(x)=x**n\n",
    "then\n",
    "df(x)/dx=nx**n-1\n",
    "\n",
    "    Chain Rule\n",
    "\n",
    "The chain rule is used for calculating the derivative of composite functions. The chain rule can also be expressed in Leibniz‚Äôs notation as follows:\n",
    "\n",
    "If a variable z depends on the variable y, which itself depends on the variable x, so that y and z are dependent variables, then z, via the intermediate variable of y, depends on x as well. This is called the chain rule and is mathematically written as,\n",
    "\n",
    "dz/dx=dz/dy.dy/dx\n",
    "\n",
    "\n",
    "Let us understand it through an example:\n",
    "if y=x**2 and x-z**2\n",
    "Then the derativeof y with respect to Z can be calculated with chain rule\n",
    "dy/dx=2x\n",
    "dx/dz=2z\n",
    "so \n",
    "dy/dz=2x2z\n",
    "\n",
    "\n",
    "    Using the Power and Chain Rule for derivatives, let‚Äôs calculate how Cost function changes relative to m and c. This deals with the concept of partial derivatives which says that if there is a function of two variables, then to find the partial derivative of that function w.r.t to one variable, treat the other variable as constant. This will be more clear with an example:\n",
    "\n",
    "Partial Derivatives\n",
    "f(x,y)=x**4+y**7\n",
    "partial derivative of function with respect to x\n",
    "df/dx=4x**3+0\n",
    "terating y as a constant\n",
    "\n",
    "and partial derivative of the function  with respect to x\n",
    "\n",
    "df/dx=0+7y**6\n",
    "treating y as a constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat are residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual is the difference between the orginal value(y)and the predicted value(y hat).This is used to check how good is the prediction made by the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is a term that is a measure of the strength of a linear relationship between two quantitative variables (e.g., height, weight). Generally there are 3 types of corelations positive andd negative\n",
    "\n",
    "Positive correlation is a relationship between two variables in which both variables move in the same direction. This is when one variable increases while the other increases and visa versa. \n",
    "Whilst negative correlation is a relationship where one variable increases as the other decreases, and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity reffers to the situation where where one or more predictor variables show a corelation to one another. This will reduce the accuracy of the estimation coefficient and will increase the standard error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tHow to detect multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common methods to detect multicollinearity is by using correlation heat maps where a score above 0.6 is normally considred as highly correlated. Variable inflation Rate is another method, a score of 5 is the threshold for this, any score above 5 is considred as highly corelated.\n",
    "principle component analysis is also used to detect multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tWhat are the remedies for multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose from any of the following steps as a remedie for multicollinearity\n",
    "1, Do Nothing- If there is not much effect in the model performance\n",
    "2, Drop column- Drop one of the column from the corelated ones\n",
    "3, Creat new feature- Derive a newe feature from the combination of the corellated ones and drop the corelaterd features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tWhat is the R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-Squared statistics is used to check the messuare of fit of model performance. the value can range from 0-1 where is a low and 1 is considr=ered as good performance. for example a model with R-Squared score of 0.9 is said to be good model whic explains 90% of variance explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tWhat is an adjusted R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number of features increase the Rsquared score will not be a accurate prediction of the model performance. To overcome this issue adjusted RSquared was introduced.\n",
    "Adjusted RSquared take ionto considration the number of features as well for checking the performance of the model.\n",
    "adjusted RSquared=1-(1-R**2)*(N-1)/N-P-1\n",
    "where\n",
    "N=Sample Size\n",
    "P=Number of predictors\n",
    "R**2= Rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.\tWhy do we use adj R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared show a higher score when the number of predictor variable increase even thought the actual model performance is not good\n",
    "So we introduced Adjusted RSquared whcih take into considration the number of predictors.\n",
    "adjusted RSquared=1-(1-R**2)*(N-1)/N-P-1\n",
    "where\n",
    "N=Sample Size\n",
    "P=Number of predictors\n",
    "R**2= Rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tWhy adj R-squared decreases when we use incompetent variables?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For adjusted R2 to increase, the  addition of a new variable has to explain more additional variation in the data than would be expected from an unrelated variable. So when we add incompetent variables it's possible to add less than it would be expected to, just by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tHow to interpret a Linear Regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression model, the most commonly known evaluation metrics include:\n",
    "\n",
    "1,R-squared (R2), which is the proportion of variation in the outcome that is explained by the predictor variables. In multiple regression models, R2 corresponds to the squared correlation between the observed outcome values and the predicted values by the model. The Higher the R-squared, the better the model.\n",
    "\n",
    "2,Root Mean Squared Error (RMSE), which measures the average error performed by the model in predicting the outcome for an observation. Mathematically, the RMSE is the square root of the mean squared error (MSE), which is the average squared difference between the observed actual outome values and the values predicted by the model. So, MSE = mean((observeds - predicteds)^2) and RMSE = sqrt(MSE). The lower the RMSE, the better the model.\n",
    "\n",
    "3, Residual Standard Error (RSE), also known as the model sigma, is a variant of the RMSE adjusted for the number of predictors in the model. The lower the RSE, the better the model. In practice, the difference between RMSE and RSE is very small, particularly for large multivariate data.\n",
    "\n",
    "4, Mean Absolute Error (MAE), like the RMSE, the MAE measures the prediction error. Mathematically, it is the average absolute difference between observed and predicted outcomes, MAE = mean(abs(observeds - predicteds)). MAE is less sensitive to outliers compared to RMSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tWhat is the difference between fit, fit_transform and predict methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To center the data (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation:\n",
    "\n",
    "x‚Ä≤=x‚àíŒºœÉ\n",
    "\n",
    "You do that on the training set of data. But then you have to apply the same transformation to your testing set (e.g. in cross-validation), or to newly obtained examples before forecast. But you have to use the exact same two parameters Œº and œÉ(values) that you used for centering the training set.\n",
    "Hence, every sklearn's transform's fit() just calculates the parameters (e.g. Œº and œÉ in case of StandardScaler) and saves them as an internal object's state. Afterwards, you can call its transform() method to apply the transformation to any particular set of examples.\n",
    "\n",
    "fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x, while also returning the transformed x‚Ä≤. Internally, the transformer object just calls first fit() and then transform() on the same data.\n",
    "\n",
    "We can predict the class for new data instances using our finalized classification model in scikit-learn using the predict() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tHow do you plot the least squared line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "data=pd.read_csv('Advertising.csv')\n",
    "feature_cols = ['TV']\n",
    "X = data[feature_cols]\n",
    "y = data.sales\n",
    "\n",
    "# follow the usual sklearn pattern: import, instantiate, fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# print intercept and coefficients\n",
    "print(lm.intercept_)\n",
    "print(lm.coef_)\n",
    "#Data frame for testing\n",
    "X_new = pd.DataFrame({'TV': [50]})\n",
    "\n",
    "lm.predict(X_new)\n",
    "\n",
    "X_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})\n",
    "X_new.head()\n",
    "#prediction\n",
    "preds = lm.predict(X_new)\n",
    "preds\n",
    "\n",
    "#Plotting \n",
    "# first, plot the observed data\n",
    "data.plot(kind='scatter', x='TV', y='sales')\n",
    "\n",
    "# then, plot the least squares line\n",
    "plt.plot(X_new, preds, c='red', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19.\tWhat are Bias and Variance? What is Bias Variance Trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance- is the amount by which the predicted y would change if we estimate it with a different tranning data set. Since the tranning data are used to fit the learning method different trannig will result in different prediction but this difference should not be too much. However if a the varince is high  then small change in the traning data will result in a large change in the prediction\n",
    "Bias- is the error that is introduced by approximating a real life problem, which may be extremely complicated, by a much simpler problem.\n",
    "\n",
    "Bias-Variance Trade off\n",
    "It is easy to find obtain a method with extremely low bias but high variance or a method with high bias and low variance  \n",
    "A good learning model need low bias and low vairance.We wont be able to find a point were both bias and variance are low so we need to make a trade off, like not the lowest bias/Variance point but little bit linent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.\tWhat is the null and alternate hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null hypothesis (H0)\n",
    "    The null hypothesis states that a population parameter (such as the mean, the standard deviation, and so on) is equal to a hypothesized value. The null hypothesis is often an initial claim that is based on previous analyses or specialized knowledge. \n",
    "    \n",
    "Alternative Hypothesis (H1)\n",
    "    The alternative hypothesis states that a population parameter is smaller, greater, or different than the hypothesized value in the null hypothesis. The alternative hypothesis is what you might believe to be true or hope to prove true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21.\tWhat is multiple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression (MLR),  is a statistical technique that uses more than one explanatory variables to predict the outcome of a response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\tWhat is the OLS method? Derive the formulae used in the OLS method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is dwefined as \n",
    "yi= Œ± + Œ≤xi+ Œµi\n",
    "Where Œµi is the error term, and Œ±, Œ≤ are the true (but unobserved) parameters of the regression. The parameter Œ≤ represents the variation of the dependent variable when the independent variable has a unitary variation: namely, if my parameter is equal to 0.75, when my x increases by 1, my dependent variable will increase by 0.75. On the other hand, the parameter Œ± represents the value of our dependent variable when the independent one is equal to zero.\n",
    "Now, the idea of Simple Linear Regression is finding those parameters Œ± and Œ≤ for which the error term is minimized. To be more precise, the model will minimize the squared errors: indeed, we do not want our positive errors to be compensated by the negative ones, since they are equally penalizing for our model.This procedure is called Ordinary Least Squared error ‚Äî OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\tWhat is the p-value? How does it help in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-value or probability value or asymptotic significance is a probability value for a given statistical model that, if the null hyothesis is true, a set of statistical observations more commonly known as the statistical summary is greater than or equal in magnitude to the observed results. \n",
    "\n",
    "Removal of different features from the dataset will have different effects on the p-value for the dataset. We can remove different features and measure the p-value in each case. These measured p-values can be used to decide whether to keep a feature or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.\tHow to handle categorical values in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy Coding: Dummy coding is a commonly used method for converting a categorical input variable into continuous variable. ‚ÄòDummy‚Äô, as the name suggests is a duplicate variable which represents one level of a categorical variable. Presence of a level is represent by 1 and absence is represented by 0. For every level present, one dummy variable will be created.\n",
    "\n",
    "Convert to number: As discussed above, some ML libraries do not take categorical variables as input. Thus, we convert them into numerical variables. Below are the methods to convert a categorical (string) input to numerical nature:\n",
    "\n",
    "    Label Encoder: It is used to transform non-numerical labels to numerical labels (or nominal categorical variables). Numerical labels are always between 0 and n_classes-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25.\tWhat is regularization, and why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularizations are techniques used to reduce the error by fitting a function appropriately on the given training set and avoid overfitting.\n",
    "Some time the model will not learn from the data set it just try to connect the data point or we can say it try to byheart the data and gives a very high accuracy but will perform very badly when we use the test data set. This condition is called over fitting. To find out if the model is overfited we do reguarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.\tExplain Ridge Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression penalizes the model based on the sum of squares of magnitude of the coefficients. The regularization term is given by\n",
    "\n",
    "regularization=ùúÜ‚àó‚àë|ùõΩ2ùëó|\n",
    "\n",
    "Where, Œª is the shrinkage factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.\tExplain Lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO regression penalizes the model based on the sum of magnitude of the coefficients. The regularization term is given by\n",
    "\n",
    "regularization=ùúÜ‚àó‚àë|ùõΩùëó|\n",
    "\n",
    "Where, Œª is the shrinkage factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.\tExplain Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net first emerged as a result of critique on lasso, whose variable selection can be too dependent on data and thus unstable. The solution is to combine the penalties of ridge regression and lasso to get the best of both worlds. Elastic Net aims at minimizing the following loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.\tWhy do we do a train test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create a model we need data to teach/train, the model for this we use traning data and once we are done with the tranning we use that model to predict on test data to see how good the model is. We take bot test and train dat from the same pool so that we know that the data is of same characteristics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30.\tWhat is polynomial regression? When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the polynomial regression to fit a polynomial line so that we can achieve a minimum error or minimum cost function. \n",
    "We use it when there is a  relationship between the dependent and independent variables bit the relationship is no linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.\tExplain the steps for GCP deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON GCP website\n",
    "\n",
    "Login to your GCP account\n",
    "AM --> admin(highlighted) -->click manage resources\n",
    "App Engin --> click Dashboard--> select python\n",
    "Download GoogleCloudSDKInstaller.exe\n",
    "Create New Project\n",
    "\n",
    "In your local machine\n",
    "\n",
    "1,In the project folder create \"app.yaml\" open it in text editor and enter runtime: python37\n",
    "2, Create a ‚Äòrequirements.txt‚Äô file by opening the command prompt/anaconda prompt, navigate to the project folder and enter the command ‚Äòpip freeze > requirements.txt‚Äô. It is recommended to use separate environments for different projects.  \n",
    "3, The app file should be named main.py\n",
    "4,open CMD from the project folder\n",
    "5, type -->gcloud init\n",
    "this will display the projects select the one created (use the numeric code)\n",
    "6, enter command-->gcloud app adeploy app.yaml -- project name\n",
    "7 next screen select the region\n",
    "8, confirm the deployment press \"y\"\n",
    "9, get the link to your app\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. What difficulties did you face in cloud deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had faced problem with requirements.txt. The file i created did not work in GCP so i copied the file provided by ineuron then the web page got loaded but was not doing the predition. So i again created a requirements.txt file and did mix and match with the requirements.txt from ineuron\n",
    "and at it worked fine with the following requirements.txt entries\n",
    "\n",
    "certifi==2020.6.20\n",
    "Click==7.1.2\n",
    "Flask==1.1.2\n",
    "Flask-Cors==3.0.8\n",
    "gunicorn==20.0.4\n",
    "itsdangerous==1.1.0\n",
    "Jinja2==2.10.3\n",
    "joblib==0.14.0\n",
    "MarkupSafe==1.1.1\n",
    "numpy==1.17.4\n",
    "scikit-learn==0.23.1\n",
    "scipy==1.5.2\n",
    "six==1.15.0\n",
    "sklearn==0.0\n",
    "threadpoolctl==2.1.0\n",
    "Werkzeug==1.0.1\n",
    "wincertstore==0.2\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
